---
title: "News"
permalink: /news/
author_profile: true
---

<span class='anchor' id='news'></span>

# Full News List



* [2025.01] &nbsp;🎉🎉 Our paper "Hot-pluggable Federated Learning: Briding General and 
Personalized FL via Dynamic Selection" is selected at <strong><font color=red>ICLR 2025</
font></strong>. This paper proposes a selective federated learning approach to integate 
personalized modules into general federated learning. ([Paper](https://openreview.net/
forum?id=B8akWa62Da))
* [2025.01] &nbsp;🎉🎉 Our paper "STBLLM: Breaking the 1-Bit Barrier with Structured 
Binary LLMs" is selected at <strong><font color=red>ICLR 2025</font></strong>. This 
paper introduces STBLLM, a novel approach that breaks the 1-bit barrier in language 
models by leveraging Structured Binary LLMs. ([Paper](https://openreview.net/forum?
id=6XUSDvBFkV))
* [2025.01] &nbsp;🎉🎉 Our paper "The Lottery LLM Hypothesis, Rethinking What Abilities 
Should LLM Compression Preserve?" is selected as <strong><font color=red>ICLR Blogpost 
2025</font></strong>. This blog proposes a lottery LLM hypothesis suggesting that for a 
given LLM and task, there exists a smaller lottery LLM capable of producing the same 
performance with the original LLM with the assistances of multi-step reasoning and 
external tools. ([Paper](https://openreview.net/forum?id=Vq3noyDfJI))
* [2025.01] &nbsp;🎉🎉 Our paper "Can LLM Simulations Truly Reflect Humanity? A Deep 
Dive." is selected as <strong><font color=red>ICLR Blogpost 2025</font></strong>. This 
blog rethinks LLM-based simulations by emphasizing both their limitations and the 
necessities for advancing LLM simulations. It offer actionable insights and strategies 
for enhancing the applicability of LLM simulations in human society in the future. 
([Paper](https://openreview.net/forum?id=dMrhmQdrdW))
* [2025.01] &nbsp;🎉🎉 Our paper "ParZC: Parametric Zero-Cost Proxies for Efficient NAS.
" is selected at <strong><font color=red>AAAI 2025</font></strong> for <strong><font 
color=red>Oral Presentation</font></strong>!
* [2024.12] &nbsp;🎉🎉 Our paper "ParZC: Parametric Zero-Cost Proxies for Efficient NAS.
" is accepted at <strong><font color=red>AAAI 2025</font></strong>! Parametric Zero-Cost 
Proxies (ParZC) method improves zero-shot Neural Architecture Search by addressing 
unequal node importance and using novel techniques for uncertainty estimation and 
architecture ranking. ([paper]() and [codes]() will come soon...)
* [2024.11] &nbsp;🎉🎉 I'm selected as the <strong><font color=red>Top Reviewer of 
NeurIPS 2024</font></strong> for both main and D&B tracks ([Link](https://nips.cc/
Conferences/2024/ProgramCommittee#top-reviewers)).
* [2024.10] &nbsp;🎉🎉 Our paper "Hot Pluggable Federated Learning." has been selected 
by the FL@FM-NeurIPS'24 workshop to receive the <strong><font color=red>Outstanding 
Student Paper Award</font></strong>!. Congratulations to all co-authors!
* [2024.10] &nbsp;🎉🎉 Our paper "FSMoE: A Flexible and Scalable Training System for 
Sparse Mixture-of-Experts Models." is accepted In <strong><font color=red>ASPLOS 2025</font></strong>! In this paper, we design and implement a new training system modularizes 
various operators in the entire MoE model, providing more fine-grained computation and 
communication scheduling, and achieving better computation communication overlap through 
appropriate gradient segmentation.. ([paper]() and [codes]() will come soon...)
* [2024.09] &nbsp;🎉🎉 Our paper "Hot Pluggable Federated Learning." is accepted at 
Workshop Federated Foundation Models@NeurIPS 2024  as an <strong><font color=red>Oral</font></strong> paper!. In this paper, we propose a new method to regard model heads as 
pluggable modules appended after the model backbone. ([paper]() and [codes]() will come 
soon...)
* [2024.09] &nbsp;🎉🎉 Our paper "FuseFL: One-Shot Federated Learning through the Lens 
of Causality with Progressive Model Fusion." is accepted at <strong><font 
color=red>NeurIPS 2024 as a Spotlight</font></strong> paper ! This work identifies the 
cause of low performance of one-shot FL, and proposes FuseFL to progressively train and 
fuses DNN model following a bottom-up manner, reducing communication costs to an 
extremely low degree. ([paper]() and [codes]() will come soon...)
* [2024.09] &nbsp;🎉🎉 Our paper "Discovering Sparsity Allocation for Layer-wise Pruning 
of Large Language Models." is accepted at <strong><font color=red>NeurIPS 2024</font></
strong>. In this paper, we present a new method for optimizing layerwise sparsity 
allocation in large language models. ([paper]() and [codes]() will come soon...)
* [2024.09] &nbsp;🎉🎉 Our paper "Should We Really Edit Language Models? On the 
Evaluation of Edited Language Models." is accepted at <strong><font color=red>NeurIPS 
2024</font></strong>. In this paper, we benchmark the methods of editing LLMs and see 
how they influence LLM performance. ([paper]() and [codes]() will come soon...)
* [2024.09] &nbsp;🎉🎉 Our paper "LPZero: Language Model Zero-cost Proxy Search from 
Zero." is accepted at <strong><font color=red>EMNLP 2024 (Findings)</font></strong>. In 
this paper, we propose LPZero, which can automatically design zero-cost proxies for NLP 
tasks. It uses genetic programming to find optimal symbolic equations, outperforming 
human-designed proxies in ranking consistency. ([paper]() and [codes]() will come 
soon...)
* [2024.06] &nbsp;🎉🎉 Our paper "Bandwidth-Aware and Overlap-Weighted Compression for 
Communication-Efficient Federated Learning." is accepted at <strong><font color=red>ICPP 
2024</font></strong>. This work finds that the overlapness between indexes of compressed 
client model parameters demonstrates important information that can be utilized to 
adjust averging weights. ([paper]())
* [2024.05] &nbsp;🎉🎉 Our paper "Pruner-Zero: Evolving Symbolic Pruning Metric From 
Scratch for Large Language Models." is accepted at <strong><font color=red>ICML 2024</
font></strong>. This work finds new pruning metric to prune LLMs to achieve SOTA 
performance under the same sparsity ratio. ([paper](https://arxiv.org/abs/2406.02924))
* [2024.03] &nbsp;🎉🎉 VMRNN is available. This work proposes the VMRNN cell, a new 
recurrent unit that integrates the strengths of Vision Mamba blocks with LSTM. We 
construct a network centered on VMRNN cells to tackle spatiotemporal prediction tasks 
effectively. ([paper](https://arxiv.org/abs/2403.16536), [code](https://github.com/
yyyujintang/VMRNN-PyTorch))
* [2024.01] &nbsp;🎉🎉 Our paper "FedImpro: Measuring and Improving Client Update in 
Federated Learning" is accepted at <strong><font color=red>ICLR 2024</font></strong>. 
([paper](https://arxiv.org/pdf/2402.07011.pdf),[code](https://github.com/wizard1203/
FedImpro)). This work trains the high-level neural network on reconstructed feature 
distributions, to accelerate FL training and enhance the model performance.
* [2024.01] &nbsp;🎉🎉 Our paper "Towards Efficient and Reliable LLM Serving: A 
Real-World Workload Study" is available. ([paper](https://arxiv.org/pdf/2401.17644.pdf)),
[code](https://github.com/HPMLL/BurstGPT). This paper introduces the first real-world 
trace dataset of LLM serving workloads, detailing user, system, and LLM behaviors. Many 
new insights of GPT services are found in this work.
* [2023.10] &nbsp;🎉🎉 Our paper "Reliable and Efficient In-Memory Fault Tolerance of 
Large Language Model Pretraining" is available. ([paper](https://arxiv.org/pdf/2310.
12670.pdf)). This work desings an in-memory fault-tolerance framework for large-scale 
LLM pretraining.
* [2023.09] &nbsp;🎉🎉 Our paper "FusionAI: Decentralized Training and Deploying LLMs 
with Massive Consumer-Level GPUs" is available. ([paper](https://arxiv.org/abs/2309.
01172)). This paper envisions a decentralized system unlocking the potential vast 
untapped consumer-level GPUs in pre-training, inference and fine-tuning of LLMs with 
privacy protection.
* [2023.02] &nbsp;🎉🎉 Our paper "FedML Parrot: A Scalable Federated Learning System via 
Heterogeneity-aware Scheduling on Sequential and Hierarchical Training" is available. 
([paper](https://arxiv.org/pdf/2303.01778.pdf)). This work aims to democratize 
simulating large-scale FL experiments. BTW, our open-source FL framework [FedML](https://
github.com/FedML-AI/FedML) has reached 2.6k stars at github.
* [2022.12] &nbsp;🎉🎉 Our paper "GossipFL: A Decentralized Federated Learning Framework 
with Sparsified and Adaptive Communication" has been accepted by <strong><font 
color=red>IEEE TPDS</font></strong>. ([paper](https://ieeexplore.ieee.org/document/
9996127)).
* [2022.10] &nbsp;🎉🎉 Our paper “NAS-LID: Efficient Neural Architecture Search with 
Local Intrinsic Dimension” is accepted at <strong><font color=red>AAAI 2023</font></
strong>. ([paper](https://arxiv.org/abs/2211.12759)).
* [2022.05] &nbsp;🎉🎉 Our paper "Virtual Homogeneity Learning: Defending against Data 
Heterogeneity in Federated Learning" is accepted at <strong><font color=red>ICML 2022</
font></strong>. ([paper](https://proceedings.mlr.press/v162/tang22d.html), [code]
(https://github.com/wizard1203/VHL)).
* [2022.03] &nbsp;🎉🎉 Our paper "FedCV: A Federated Learning Framework for Diverse 
Computer Vision Tasks" is accepted to <strong><font color=red>FL-AAAI workshop '22 as 
Oral Presentation</font></strong>. [paper](https://arxiv.org/pdf/2111.11066.pdf).
* [2021.07] &nbsp;🎉🎉 Our paper "Data Resampling for Federated Learning with Non-IID 
Labels." is accepted to FTL-IJCAI workshop '21 [paper](https://federated-learning.org/
fl-ijcai-2021/FTL-IJCAI21_paper_3.pdf).
* [2020.10] &nbsp;🎉🎉 Our survey paper "A Quantitative Survey of Communication 
Optimizations in Distributed Deep Learning" \[[code](https://github.com/HKBU-HPML/
ddl-benchmarks), [paper](https://arxiv.org/abs/2005.13247)\] has been accepted by *IEEE 
Network Magazine*.

